---
title: CPU 矩阵乘法前人优化路径复现与探案：从 perf 数据到 AVX-512 真相
summary: 如何用系统化的方式理解产品设计中的对话与反馈循环。
pubDate: 2025-10-01
# tags:
#   - 产品设计
#   - 用户体验
---

import { Image } from 'astro:assets'
import hero from '../../assets/posts/Basic_MMM.png'

> 知其然到知其所以然

<Image
  src={hero}
  alt="示意图"
  widths={[480, 768, 1200]}
  sizes="(min-width: 768px) 768px, 100vw"
  class="rounded-xl border border-slate-200 shadow-sm"
/>
# 引言
在高性能计算领域，通用矩阵乘法（GEMM）的优化是一个永恒的议题。它看似简单的三层循环背后，蕴藏着对现代CPU体系结构理解的深度考验，从缓存层次、内存带宽到指令级并行，每一处都可能是性能的瓶颈所在。对于追求极致性能的开发者而言，深入探索GEMM的优化不仅是一项技术挑战，更是一次通往计算机系统底层的旅行。

幸运的是，前行之路上已有灯塔指引。近年来，有很多杰出的技术博客和论文为我们绘制了详尽的优化地图，它们从不同的视角出发，引领我们一步步将一个朴素的实现推向性能极限。在开始自己的探索之前，有必要向部分卓越的工作致以敬意，并梳理它们的贡献。

| 文章出处 & 链接                                                                                                                                        | 关键贡献与Insights                                                                                                                       |
| -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [**Fast MMM on CPU**](https://siboehm.com/articles/22/Fast-MMM-on-CPU) by S. Boehm                                           | <li>通过清晰的、可复现的步骤，展示了每次优化（如循环重排、Tiling）带来的直观性能增益。</li><li>强调了理论与实践的结合，每一步优化都有坚实的性能数据作为支撑。</li><li>为初学者和中级实践者提供了一条极其友好的学习路径。</li> |
| [**CPU Matrix Multiplication from Scratch**](https://blog.pebblesandweeds.com/cpu_matmul_blog.html) by V. Rejichev                | <li>系统性地将向量化（SIMD）分解为五大要素：指令集、数据对齐、预取、转置和循环展开。</li><li>提供了具体的AVX2 intrinsic代码示例，深入探讨了SIMD编程的实践细节。</li><li>其核心价值在于对SIMD这一关键优化技术的“解构式”教学。</li> |
| [**Advanced Matrix Multiplication Optimization**](https://salykova.github.io/gemm-cpu) by A. Salykova              | <li>引入并实践了BLIS（BLAS-like Library Instantiation Software）框架的设计哲学。</li><li>阐述了“微内核”（micro-kernel）在最大化寄存器复用中的核心作用。</li><li>展示了针对多级缓存的阻塞（Blocking）策略和内存打包（Packing）的必要性，提供了一个可扩展的高性能架构。</li> |

站在这些巨人的肩膀上，任何试图在该领域撰写新文章的尝试都显得野心勃勃。上述文章已经从算法、架构和工程实践等多个维度，对GEMM优化进行了精妙的阐述。作为一个仍在学习路上的探索者，我无意也无力提出一种全新的、超越前人的优化算法。

然而，在学习和复现这些工作的过程中，我发现了一个可以补充的视角。这些文章卓越地回答了“**做什么（What）**”和“**为什么（Why）**”——我们应该做什么样的优化，以及为什么这些优化在理论上是有效的。但对于实践者而言，一个更为根本的问题或许是：“**如何确证（How to Verify）**”。当我们的代码并未如预期般加速时，我们如何通过量化数据，**诊断出性能瓶颈究竟是缓存未命中、指令依赖，还是别的什么？**

因此，本文的目的并非提出一种新的优化技术，而是试图进行一次**方法论上的探索**。我们将扮演“性能侦探”的角色，使用 `perf`、`Compiler Explorer` 等工具链作为我们的“放大镜”和“指纹分析工具”，对GEMM的优化过程进行一次Forensic Analysis。

我们将从最朴素的实现出发，不仅应用循环重排、向量化等优化，更重要的是，在每一步之后，我们将深入分析性能剖析工具给出的“证据”——例如缓存未命中率的变化、CPU指令周期的消耗、以及编译器生成的汇编代码的差异。此外，我们将把目光投向更为现代的AVX-512指令集，探究其为微内核设计带来的新机遇与挑战。

本文希望为那些不仅想知道“怎么做”，更想知道“如何观察和验证”的读者，提供一套可操作的诊断思路。让我们开始这次探案之旅。


好的，我们现在正式开始“探案”。这是第一幕的第一部分，我们将聚焦于“案发现场”——朴素（Naive）实现的矩阵乘法，并使用工具收集初步的“法证”证据。

---

### 第一幕：追踪头号嫌犯——Cache Miss

从一个基本的嵌套 for 循环开始

#### **代码：`gemm_naive.cpp`**

实现采用 `(i, j, k)` 的循环顺序，直接对应数学公式 `C[i][j] = Σ A[i][k] * B[k][j]` 。所有矩阵均以一维向量形式存储，遵循行主序（Row-Major Order）内存布局，用于计算两个N x N的单精度浮点矩阵`A`和`B`的乘积，结果存入`C`。

```cpp
// Function to perform naive matrix multiplication
// C = A * B
// Loops order: i, j, k
void gemm_naive(const std::vector<float>& A, const std::vector<float>& B, std::vector<float>& C, int N) {
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < N; ++j) {
            float sum = 0.0f;
            for (int k = 0; k < N; ++k) {
                // C(i, j) is the dot product of row i of A and column j of B
                sum += A[i * N + k] * B[k * N + j];
            }
            C[i * N + j] = sum;
        }
    }
}
```

#### **侦查工具与操作方法**

现在，我们将以性能侦探的身份，对这个“案发现场”进行勘查。你需要准备好Linux环境下的`g++`编译器和`perf`工具。

**第一步：保存并编译代码**

将上面的代码保存为文件 `gemm_naive.cpp`。然后打开终端，使用以下命令进行编译：

```bash
g++ -O3 -o gemm_naive gemm_naive.cpp
```

*   **`g++`**: 我们使用的C++编译器。
*   **`-O3`**: 这是一个重要的编译选项，它会开启大部分编译器自带的优化。我们必须加上它，以确保我们分析的是算法本身的瓶颈，而不是编译器没有尽力。
*   **`-o gemm_naive`**: 指定编译后生成的可执行文件的名字。

**第二步：建立性能基准**

首先，让我们直接运行程序，看看它的性能有多糟糕。这是我们后续所有调查的起点。

```bash
./gemm_naive
```

你将会看到类似下面的输出（具体数值取决于你的CPU）：

```
Matrix Size: 512x512
Elapsed Time: 1.854 s
Performance: 0.144 GFLOPS
```

记录下这个 GFLOPS 值。它非常低，这证实了“性能悲剧”的存在。

**第三步：收集法证证据**

现在，到了最关键的一步。我们将使用`perf`工具，像X光机一样透视程序在CPU上运行时的内部情况。我们将重点关注与缓存和CPU周期相关的硬件事件。

在终端中运行以下命令：

```bash
perf stat -e L1-dcache-load-misses,cache-misses,cycles,instructions ./gemm_naive
```

*   **`perf stat`**: `perf`工具的统计模式。
*   **`-e`**: 用于指定我们感兴趣的硬件事件（events）。
*   **`L1-dcache-load-misses`**: **核心证据**。CPU尝试从最快的一级数据缓存（L1d cache）中读取数据但失败的次数。这个比例越高，说明缓存命中率越低。
*   **`cache-misses`**: 所有级别缓存（L1, L2, L3）的总未命中率。这是一个宏观指标。
*   **`cycles`, `instructions`**: CPU执行的总时钟周期数和总指令数。这两个值可以用来计算**CPI（Cycles Per Instruction）**，即执行平均一条指令需要多少个时钟周期。

**第四步：解读“法医报告”**

执行完`perf`命令后，你会得到一份详细的报告。它看起来可能像这样：

```
 Performance counter stats for './gemm_naive':

     268,460,838      L1-dcache-load-misses     #   66.11% of all L1-dcache-loads
     269,218,814      cache-misses              #   90.218 % of all cache refs
 5,987,123,456      cycles                    #    3.123 GHz
 1,879,456,123      instructions              #    0.31  insn per cycle

       1.917123456 seconds time elapsed
```

现在，让我们来解读这份报告：

1.  **GFLOPS 值依然很低**：程序本身的输出会再次确认这一点。
2.  **`L1-dcache-load-misses` 比例极高**：在上面的示例中，高达 **66.11%**！这意味着每三次L1缓存读取，就有两次是失败的。CPU不得不去更慢的L2、L3缓存甚至主内存中寻找数据。**这就是我们要找的第一个“指纹”，是性能问题的直接证据！**
3.  **CPI (Cycles Per Instruction) 远大于1**：我们可以手动计算 `CPI = cycles / instructions`。在示例中，`5,987,123,456 / 1,879,456,123 ≈ 3.18`。这意味着CPU平均要花费超过3个时钟周期才能执行一条指令。理想情况下，一个健康的、计算密集的程序CPI应该接近甚至小于1。这个高得离谱的CPI值表明，CPU的计算单元在绝大多数时间里都处于**“等待”**状态——等待数据从慢速内存中加载过来。

**初步结论：**

我们的初步调查取得了重大突破。所有证据都指向了同一个嫌疑人——**糟糕的内存访问模式导致了灾难性的缓存未命中**。虽然我们还没有抓到真凶，但我们已经精确地锁定了它的犯罪手法。在下一节中，我们的任务就是分析代码，找出究竟是哪部分导致了这种缓存灾难，并实施第一次抓捕。



