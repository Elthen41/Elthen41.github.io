---
title: CPU 矩阵乘法前人优化路径复现与探案：从 perf 数据到 AVX-512 真相
summary: 如何用系统化的方式理解产品设计中的对话与反馈循环。
pubDate: 2025-10-01
# tags:
#   - 产品设计
#   - 用户体验
---

import { Image } from 'astro:assets'
import hero from '../../assets/posts/Basic_MMM.png'

> 知其然到知其所以然

<Image
  src={hero}
  alt="示意图"
  widths={[480, 768, 1200]}
  sizes="(min-width: 768px) 768px, 100vw"
  class="rounded-xl border border-slate-200 shadow-sm dark:border-slate-700 dark:bg-slate-900/60"
/>
# 引言
在高性能计算领域，通用矩阵乘法（GEMM）的优化是一个永恒的议题。它看似简单的三层循环背后，蕴藏着对现代CPU体系结构理解的深度考验，从缓存层次、内存带宽到指令级并行，每一处都可能是性能的瓶颈所在。对于追求极致性能的开发者而言，深入探索GEMM的优化不仅是一项技术挑战，更是一次通往计算机系统底层的旅行。

幸运的是，前行之路上已有灯塔指引。近年来，有很多杰出的技术博客和论文为我们绘制了详尽的优化地图，它们从不同的视角出发，引领我们一步步将一个朴素的实现推向性能极限。在开始自己的探索之前，有必要向部分卓越的工作致以敬意，并梳理它们的贡献。

| 文章出处 & 链接                                                                                                                                        | 关键贡献与Insights                                                                                                                       |
| -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [**Fast MMM on CPU**](https://siboehm.com/articles/22/Fast-MMM-on-CPU) by S. Boehm                                           | <li>通过清晰的、可复现的步骤，展示了每次优化（如循环重排、Tiling）带来的直观性能增益。</li><li>强调了理论与实践的结合，每一步优化都有坚实的性能数据作为支撑。</li><li>为初学者和中级实践者提供了一条极其友好的学习路径。</li> |
| [**CPU Matrix Multiplication from Scratch**](https://blog.pebblesandweeds.com/cpu_matmul_blog.html) by V. Rejichev                | <li>系统性地将向量化（SIMD）分解为五大要素：指令集、数据对齐、预取、转置和循环展开。</li><li>提供了具体的AVX2 intrinsic代码示例，深入探讨了SIMD编程的实践细节。</li><li>其核心价值在于对SIMD这一关键优化技术的“解构式”教学。</li> |
| [**Advanced Matrix Multiplication Optimization**](https://salykova.github.io/gemm-cpu) by A. Salykova              | <li>引入并实践了BLIS（BLAS-like Library Instantiation Software）框架的设计哲学。</li><li>阐述了“微内核”（micro-kernel）在最大化寄存器复用中的核心作用。</li><li>展示了针对多级缓存的阻塞（Blocking）策略和内存打包（Packing）的必要性，提供了一个可扩展的高性能架构。</li> |

站在这些巨人的肩膀上，任何试图在该领域撰写新文章的尝试都显得野心勃勃。上述文章已经从算法、架构和工程实践等多个维度，对GEMM优化进行了精妙的阐述。作为一个仍在学习路上的探索者，我无意也无力提出一种全新的、超越前人的优化算法。

然而，在学习和复现这些工作的过程中，我发现了一个可以补充的视角。这些文章卓越地回答了“**做什么（What）**”和“**为什么（Why）**”——我们应该做什么样的优化，以及为什么这些优化在理论上是有效的。但对于实践者而言，一个更为根本的问题或许是：“**如何确证（How to Verify）**”。当我们的代码并未如预期般加速时，我们如何通过量化数据，**诊断出性能瓶颈究竟是缓存未命中、指令依赖，还是别的什么？**

因此，本文的目的并非提出一种新的优化技术，而是试图进行一次**方法论上的探索**。我们将扮演“性能侦探”的角色，使用 `perf`、`Compiler Explorer` 等工具链作为我们的“放大镜”和“指纹分析工具”，对GEMM的优化过程进行一次Forensic Analysis。

我们将从最朴素的实现出发，不仅应用循环重排、向量化等优化，更重要的是，在每一步之后，我们将深入分析性能剖析工具给出的“证据”——例如缓存未命中率的变化、CPU指令周期的消耗、以及编译器生成的汇编代码的差异。此外，我们将把目光投向更为现代的AVX-512指令集，探究其为微内核设计带来的新机遇与挑战。

本文希望为那些不仅想知道“怎么做”，更想知道“如何观察和验证”的读者，提供一套可操作的诊断思路。让我们开始这次探案之旅。


好的，我们现在正式开始“探案”。这是第一幕的第一部分，我们将聚焦于“案发现场”——朴素（Naive）实现的矩阵乘法，并使用工具收集初步的“法证”证据。

---
我将在不同的机器配置下进行测试

配置一：
- CPU: Intel Core i7-11800H（8 核 16 线程，Tiger Lake 架构）
- RAM: 32GB DDR4 3200MHz CL22
- OS: Ubuntu 22.04 LTS
- Compiler: g++ 11.4.0 

配置二：
- CPU: AMD Ryzen 9950X（16 核 32 线程，ZEN 5 架构）
- RAM: 64GB DDR5 6000 MHz CL30
- OS: Ubuntu 22.04 LTS
- Compiler: g++ 11.4.0 
### 第一幕：性能杀手——Cache Miss

从一个基本的嵌套 for 循环开始

#### **代码：`gemm_naive.cpp`**

实现采用 `(i, j, k)` 的循环顺序，直接对应数学公式 `C[i][j] = Σ A[i][k] * B[k][j]` 。所有矩阵均以一维向量形式存储，遵循行主序（Row-Major Order）内存布局，用于计算两个N x N的单精度浮点矩阵`A`和`B`的乘积，结果存入`C`。

```cpp
// Function to perform naive matrix multiplication
// C = A * B
// Loops order: i, j, k
void gemm_naive(const std::vector<float>& A, const std::vector<float>& B, std::vector<float>& C, int N) {
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < N; ++j) {
            float sum = 0.0f;
            for (int k = 0; k < N; ++k) {
                // C(i, j) is the dot product of row i of A and column j of B
                sum += A[i * N + k] * B[k * N + j];
            }
            C[i * N + j] = sum;
        }
    }
}
```

#### **侦查工具与操作方法**

我们将使用 `perf` 工具集，它是Linux内核自带的性能分析工具，能够访问处理器的性能监控单元（PMU），从而提供底层的硬件事件计数。你需要准备好Linux环境下的`g++`编译器和`perf`工具。

**第一步：保存并编译代码**

将上面的代码保存为文件 `gemm_naive.cpp`。然后打开终端，使用以下命令进行编译：

```bash
g++ -O3 -o gemm_naive gemm_naive.cpp
```

*   **`g++`**: 我们使用的C++编译器。
*   **`-O3`**: 这是一个重要的编译选项，它会开启大部分编译器自带的优化。我们必须加上它，以确保我们分析的是算法本身的瓶颈，而不是编译器没有尽力。
*   **`-o gemm_naive`**: 指定编译后生成的可执行文件的名字。

**第二步：建立性能基准**

首先，让我们直接运行程序，看看它的性能表现。
```bash
./gemm_naive
```

你将会看到类似下面的输出：

```
Matrix Size: 256x256          Matrix Size: 512x512           Matrix Size: 1024x1024
Elapsed Time: 0.0150155 s     Elapsed Time: 0.131208 s       Elapsed Time: 2.30336 s
Performance: 2.23465 GFLOPS   Performance: 2.04588 GFLOPS    Performance: 0.932325 GFLOPS
```
<div class="caption">配置一性能表现</div>

**第三步：收集法证证据**

现在，到了最关键的一步。我们将使用`perf`工具，像X光机一样透视程序在CPU上运行时的内部情况。我们将重点关注与缓存和CPU周期相关的硬件事件。

在终端中运行以下命令：

```bash
perf stat -e L1-dcache-load-misses,cache-misses,cycles,instructions ./gemm_naive
```

*   **`perf stat`**: `perf`工具的统计模式。
*   **`-e`**: 用于指定我们感兴趣的硬件事件（events）。
*   **`L1-dcache-load-misses`**: **核心证据**。CPU尝试从最快的一级数据缓存（L1d cache）中读取数据但失败的次数。这个比例越高，说明缓存命中率越低。
*   **`cache-misses`**: 所有级别缓存（L1, L2, L3）的总未命中率。这是一个宏观指标。
*   **`cycles`, `instructions`**: CPU执行的总时钟周期数和总指令数。这两个值可以用来计算**CPI（Cycles Per Instruction）**，即执行平均一条指令需要多少个时钟周期。

**第四步：解读“法医报告”**

执行完`perf`命令后，你会得到一份详细的报告。它看起来可能像这样：

```
 Performance counter stats for './gemm_naive':

     268,460,838      L1-dcache-load-misses     #   66.11% of all L1-dcache-loads
     269,218,814      cache-misses              #   90.218 % of all cache refs
 5,987,123,456      cycles                    #    3.123 GHz
 1,879,456,123      instructions              #    0.31  insn per cycle

       1.917123456 seconds time elapsed
```

现在，让我们来解读这份报告：

1.  **GFLOPS 值依然很低**：程序本身的输出会再次确认这一点。
2.  **`L1-dcache-load-misses` 比例极高**：在上面的示例中，高达 **66.11%**！这意味着每三次L1缓存读取，就有两次是失败的。CPU不得不去更慢的L2、L3缓存甚至主内存中寻找数据。**这就是我们要找的第一个“指纹”，是性能问题的直接证据！**
3.  **CPI (Cycles Per Instruction) 远大于1**：我们可以手动计算 `CPI = cycles / instructions`。在示例中，`5,987,123,456 / 1,879,456,123 ≈ 3.18`。这意味着CPU平均要花费超过3个时钟周期才能执行一条指令。理想情况下，一个健康的、计算密集的程序CPI应该接近甚至小于1。这个高得离谱的CPI值表明，CPU的计算单元在绝大多数时间里都处于**“等待”**状态——等待数据从慢速内存中加载过来。

**初步结论：**

我们的初步调查取得了重大突破。所有证据都指向了同一个嫌疑人——**糟糕的内存访问模式导致了灾难性的缓存未命中**。虽然我们还没有抓到真凶，但我们已经精确地锁定了它的犯罪手法。在下一节中，我们的任务就是分析代码，找出究竟是哪部分导致了这种缓存灾难，并实施第一次抓捕。



**1.3.2 性能测量工具**
我们将使用 `perf` 工具集，它是Linux内核自带的性能分析工具，能够访问处理器的性能监控单元（PMU），从而提供底层的硬件事件计数。

#### **1.4 基准性能评估**

**1.4.1 宏观性能：GFLOPS**
首先，执行程序以获得其宏观性能指标——每秒十亿次浮点运算数（GFLOPS）。

```bash
./gemm_naive
```

**示例输出：**
```
Matrix Size: 512x512
Elapsed Time: 1.854 s
Performance: 0.144 GFLOPS
```
该 GFLOPS 值（0.144）将作为我们后续优化的性能基准。此数值相对于现代CPU的理论峰值（通常在数百至数千 GFLOPS）极低，表明存在严重的性能瓶颈。

**1.4.2 微观分析：硬件事件计数**
为探究性能低下的根本原因，我们使用 `perf stat` 测量关键的硬件事件。

```bash
perf stat -e L1-dcache-load-misses,cache-misses,cycles,instructions ./gemm_naive```

*   **`-e`**: 指定要监控的事件。
*   **`L1-dcache-load-misses`**: L1数据缓存读取未命中的次数。这是衡量数据局部性（Data Locality）的关键指标。
*   **`cache-misses`**: 综合所有缓存层级的未命中率。
*   **`cycles`, `instructions`**: CPU执行的时钟周期数和指令数，用于计算 CPI。

**`perf` 输出示例：**
```
 Performance counter stats for './gemm_naive':

     268,460,838      L1-dcache-load-misses     #   66.11% of all L1-dcache-loads
     269,218,814      cache-misses              #   90.218 % of all cache refs
 5,987,123,456      cycles                    #    3.123 GHz
 1,879,456,123      instructions              #    0.31  insn per cycle

       1.917123456 seconds time elapsed
```

#### **1.5 数据分析与结论**

对 `perf` 的输出数据进行分析，可以得出以下结论：

1.  **极高的L1数据缓存未命中率**: 报告显示 `L1-dcache-load-misses` 的比例高达 **66.11%**。这意味着CPU在执行加载指令时，有三分之二的情况无法在最快的一级缓存中找到所需数据。这迫使CPU流水线停顿（stall），并从更慢的L2、L3缓存或主内存中获取数据，造成了巨大的时间开销。

2.  **高CPI (Cycles Per Instruction)**: 我们可以计算 `CPI = cycles / instructions`。根据示例数据，`CPI ≈ 5.987e9 / 1.879e9 ≈ 3.18`。该值表示平均执行一条指令需要超过3个时钟周期。对于一个计算密集型任务，理想的CPI应接近1或更低（得益于超标量架构）。如此高的CPI是CPU流水线频繁停顿的直接证据，而停顿的主要原因正是等待内存数据。

**初步结论：**
量化数据明确指出，朴素GEMM实现的性能瓶颈**并非计算本身，而是内存访问**。`B[k * N + j]` 的访问模式导致了非连续的内存读取（stride为N），严重破坏了空间局部性（Spatial Locality），从而引发了灾难性的缓存未命中。这使得程序在绝大多数时间内处于内存等待状态，CPU的计算单元被严重闲置。

因此，首要的优化目标应聚焦于**改善内存访问模式，提高数据局部性，以降低缓存未命中率**。